import os
from pinecone import Pinecone as PineconeClient
import openai
from openai import OpenAI
import numpy as np

NUMBER_OF_GENERATED_QUESTIONS = 3
NUMBER_OF_SIMILAR = 2
PINECONE_KEY  = "PLACEHOLDER"
OPENAI_KEY = "PLACEHOLDER"
EMBEDDING_MODEL = "text-embedding-3-small"
PINECONE_NAMESPACE = "ns1"
PINECONE_BUCKET = "buck3"

# Get vector db and openai client references
def connections():
    pinecone = PineconeClient(api_key=PINECONE_KEY)
    open_ai_client = OpenAI(api_key=OPENAI_KEY)
    return open_ai_client, pinecone

# Get embedding for single entry
def get_embedding(text, open_ai_client, model=EMBEDDING_MODEL):
    return open_ai_client.embeddings.create(input = [text], model=model).data[0].embedding

# Given a text get NUMBER_OF_SIMILAR amount of relevent texts
# Input: (pinecone namespace to pull from, list of queries to match, openai client reference, pinecone client reference)
# Output: (list of relevent texts, list of relevent embeddings)
def get_batch_similar(queries, open_ai_client, pinecone, namespace, bucket):
    response_list = []
    embeds_list = []
    for i in range(len(queries)):
        embed_question = get_embedding(queries[i], open_ai_client, model='text-embedding-3-small')
        query_results1 = pinecone.Index(bucket).query(
            namespace=namespace,
            vector=embed_question,
            top_k=NUMBER_OF_SIMILAR,
            include_values=True,
            include_metadata=True
        )

        for j in range(NUMBER_OF_SIMILAR):
            embeds_list.append(query_results1['matches'][j]['values'])
            response_list.append(query_results1['matches'][j]['metadata']['text'])
    return response_list, embeds_list

# Does final prompting with api call to openai
# Input: (User Query)
# Output: (Generated repsonse from API)
def get_completion(prompt):
    openai.api_key = OPENAI_KEY

    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
        {"role": "system", "content": "You are a tool for security officers to understand what kind of threats exist in software."},
        {"role": "user", "content": prompt}
      ]
    )
    return response.choices[0].message.content

# Generates relevent questions from user provided query
# Input: (User Query)
# Output: (Generated relevent questions)
def question_generation(prompt):
    questions_prompt = "Can you generate "+ str(NUMBER_OF_GENERATED_QUESTIONS) + "questions that investigate different aspects of the question "
    response_format = "Can respond only with a '*' separated list starting with a *?"  
    questions_prompts = get_completion(questions_prompt + prompt + response_format)
    questions = [q.strip() for q in questions_prompts.split('*') if q.strip()]  
    return questions

# Generic similarity function
# Input: (Type of similarity formula, a text for comparison, b text for comparison)
# Output: similarity score between a and b
def similarity(simtype, a, b):
    similarity = 0 
    if (simtype == 'cosine'):
        similarity = np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))
    return similarity

# Get top k similar texts
# Input: (input text, desired number of similar texts, df of embeddings associated with text, model to use)
# Output: array of k similar texts
def get_embedding_similarity_text(text, embeds_list, text_list, client):
    # Embed Input Text
    embed_input_text = get_embedding(text, client)

    # Find similarity between input embedding and all embeddings
    results = [similarity('cosine', embed, embed_input_text) for embed in embeds_list]

    # Get top text from local context as reference
    top_index = np.argsort(results)[-1]
    return text_list[top_index]

def main_question(prompt, openai_client, pinecone, pinecone_namespace, pinecone_bucket):
    exploratory_questions = question_generation(prompt)
    #print("------------- Exploratory Questions ----------------")
    #print(exploratory_questions)
    all_responses_retrieved, all_embeds_retreived = get_batch_similar(exploratory_questions, openai_client, pinecone, pinecone_namespace, pinecone_bucket)
    #print("------------- Responses Received ----------------")
    #print(all_responses_retrieved)
    all_responses_retrieved_string = " ".join(all_responses_retrieved)
    all_responses_summary = get_completion("Summarize this text in a way that captures the different points using only 300 words: " + all_responses_retrieved_string)
    finalized_response = get_completion(prompt + " Answer the question using only the following information: " + all_responses_summary)
    return finalized_response, all_responses_retrieved, all_embeds_retreived

def follow_up_question(prompt, all_embeds_retreived, all_responses_retrieved, openai_client):
    sim_text = get_embedding_similarity_text(prompt, all_embeds_retreived, all_responses_retrieved, openai_client)
    return get_completion(prompt + " Answer this question using the given context: " + sim_text)
