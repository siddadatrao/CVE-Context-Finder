Goal Scenario: 
A CCISO or other security personnel is notified with a potential issue or inconsistent behavior.
This person wishes to know more about potential threats related to this inconsistent behavior.
CVE Context Finder gives a basic understanding of previous vulnerabilities and issues relating to what they found to provide entry level understanding with specific vulnerabilities and potential remediation. 
User asks follow up questions regarding specific vulnerabilities and their general topics to become better knowledgeable about vulnerabilities and how they relate to their security situation.

CVE Context Finder is not a tool to show exact solutions to potential issues. This is an exploration tool aimed at empowering security personnel with more context about a specific issue they may see. Since the attack vector space is constantly evolving, this tool allows security personnel to have the opportunity to keep up. As mentioned by yaâ€™ll, alert fatigue is an issue. This tool is a solution to potential research fatigue when presented with a constantly evolving field can also be an issue and having this quick tool to gain context is a benefit. 

Dataset: https://github.com/TQRG/security-patches-dataset/tree/main/data/cve-details
Contains releases from various sources about vulnerabilities found in production grade software such as linux and GraphQL. 
Each potential threat, has some description of the attack scenario and what could be done to avoid this issue if known
Contains roughly 8000 entries amounting to roughly 2,000,000 tokens

Vector Database:
Pinecone storing 3000 entries using cosine similarity
	
Models Used:
Embeddings: text-embedding-3-small from OpenAI
Text Generation: gpt-4o-mini from OpenAI


Retrieval Design:

Strengths:
LLM generated Multi Question approach allows for question asking entities to leverage LLMs to add further nuance to retrievable text
Follow up question mechanism using only a subset of past retrieved data allows for question asking entities to continue to explore current line of questioning
Query + Summarization of relevant texts enables potentially large amount of retrieved relevant texts to be represented with correct weight based on importance deemed by LLM
Summarization of relevant text is vital in removing redundancy which could create an unbalanced response
Weaknesses:
Initial latency + # of queries is high.
Design was chosen due to the mission statement. Once this initial cost is borne, follow up questions become very cheap but have rich data.

	Reasoning: 
LLM generated questions approach provides a well rounded answer for question asking entities who are at the start of an exploration journey and may not have enough context to ask questions that will result in a desirable response. This is also why the option to explore deeper without changing context is in place. The follow up question mechanism in addition to the choice to move on to new context, allows question asking entities to have onus on their explorative journey. Since the dataset involves a variety of different vulnerabilities, it is difficult for any one person to have deep understanding on all of it. This mechanism allows for users to first get a big picture understanding of potential vulnerabilities, then ask follow up questions to understand specificities.

	Other Potential Considerations:
An interesting approach would be to use the Parent-Child retrieval mechanism where a small chunk of similar text is retrieved along with its overarching parent text. In this case the parent could be another data source that has general information about the topics the vulnerabilities mention. 
I chose not to go this route because the GPT model has a pretty good understanding of the various security related topics mentioned in the dataset. If the dataset included data that was more specific to a domain or had private information, this approach could help with being able to provide valuable information during follow up questions.
Local db of relevant texts meant for follow up questions could also move to a vector db if amount of locally stored tokens increases further

Embeddings and Storage Design:
	Strengths: 
Chose text-embeddings-3-small model because of low cost and performance boost compared to text-embedding-ada-002
Since individual entries of the dataset were already in relatively small chunks, directly embedding these chunks is reasonable. From experiments with similarity searches, I could see pretty good results with text retrieval.

	Weaknesses:
Some smaller data entries could be clubbed with other related entries to prevent retrieval runs of the chatbot from having little context.
This problem is slightly offset but multi-question retrieval because multiple contexts are received and the chance for all reference text to be small is low

	Other Potential Considerations:
I experimented with running a KMeans algorithm over the initial test set with bucket sizes of 100. Then I took each bucket and clubbed five entries together to create larger chunks. I averaged the five embedding vectors and stored the five text entries associated. In future iterations, optimizing those values could lead to some interesting insights where similar CVEs could be included together to pack in more information. I ultimately decided not to include this in the final iteration, as I wanted to do more testing with parameters such as number of buckets and strategy for clubbing.
